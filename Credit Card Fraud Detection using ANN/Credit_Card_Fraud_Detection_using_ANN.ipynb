{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b1074143-76e8-42a6-8fb1-69bc34377e59",
   "metadata": {},
   "source": [
    "## ***<h1>Credit Card Fraud Detection using Single Feed-Forward Multi-Layer Perceptron ANN***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beda314f-8995-46ee-a14c-c97f8361de0c",
   "metadata": {},
   "source": [
    "***<h2>Phase 1: Importing required libraries***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31e6429b-6271-46c1-a56a-feb447a5f372",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler        # for scaling and normalising to train ANN effectively\n",
    "from sklearn.model_selection import train_test_split    # for splitting data\n",
    "from sklearn.utils import class_weight                  # helps handle class imbalance by assigning weights\n",
    "\n",
    "# for model performance evaluation\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, roc_curve, precision_recall_curve, auc  \n",
    "\n",
    "from tensorflow.keras.models import Sequential          # Sequential model type (layers stacked one after another)\n",
    "from tensorflow.keras.layers import Dense, Dropout      # Dense = fully connected layer, Dropout = regularization\n",
    "from tensorflow.keras.optimizers import Adam            # Optimizer (gradient descent variant) for training\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')                       # suppressing warnings for clean output\n",
    "\n",
    "# Set plot style\n",
    "sns.set(style=\"darkgrid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4b12c48-011f-450c-ad27-d1aac74db345",
   "metadata": {},
   "source": [
    "***<h2>Phase 2: Loading the dataset to work on***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f912f30-1eb5-4e61-8d82-c96703c73d62",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"creditcard.csv\")                                    # Loading dataset and understanding it\n",
    "\n",
    "print(\"Shape of the dataset:\", df.shape)                              # prints shape of the dataset(rows and columns)\n",
    "\n",
    "print(\"\\nDistribution of Class:\\n\", df['Class'].value_counts())       # distribution of legit and fraud classes\n",
    "\n",
    "print(\"\\nMissing values:\\n\", df.isnull().sum().sum())                 # values missing in the entire dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe9235d7-f5c2-4110-bad0-3598742fc3fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nFirst five rows of the dataset:\\n\")                          # first 5 rows\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aa194bc-85b5-4769-8fcc-f28e3c3269e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nLast five rows of the dataset:\\n\")                           # last 5 rows\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6738d722-638a-4822-9d53-bf8940d84e2e",
   "metadata": {},
   "source": [
    "***<h2>Phase 3 : EDA***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "457f954b-55f3-42c1-9bf5-6d82a253ad9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class Imbalance visualization - shows fraud vs non-fraud counts.\n",
    "# Visualizes the severe class imbalance, showing far fewer fraud cases compared to legitimate ones.\n",
    "\n",
    "plt.figure(figsize=(6,4))\n",
    "sns.countplot(x='Class', data=df)\n",
    "plt.title(\"Fraud vs Non-Fraud Transactions\")\n",
    "plt.show()\n",
    "\n",
    "fraud_cases = df[df['Class']==1]\n",
    "legit_cases = df[df['Class']==0]\n",
    "\n",
    "print(\"Fraud cases:\", len(fraud_cases))\n",
    "print(\"Legit cases:\", len(legit_cases))\n",
    "print(\"Fraud percentage: {:.4f}%\".format(len(fraud_cases)/len(df)*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da71379d-4b35-4f5d-9354-a3cf0bccc34c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fraud vs Non-Fraud transaction amounts, highlighting differences and outliers.\n",
    "\n",
    "plt.figure(figsize=(8,5))\n",
    "sns.boxplot(x=\"Class\", y=\"Amount\", data=df)\n",
    "plt.title(\"Amount by Transaction Class\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a378fc11-5a15-4c4d-b3e3-5b17fcbef1a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Heatmap showing correlations between features to identify relationships and multicollinearity.  \n",
    "\n",
    "plt.figure(figsize=(12,8))\n",
    "sns.heatmap(df.corr(), cmap=\"BuPu\", center=0, linewidths=0.1) \n",
    "plt.title(\"Correlation Heatmap of Features\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "071e79ec-3672-4d5c-92fc-d27209c56cce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# KDE plots showing distribution of PCA components (V1–V4) for fraud(1) vs non-fraud(0) to highlight feature separation.  \n",
    "\n",
    "plt.figure(figsize=(12,6))\n",
    "for i, col in enumerate(['V1','V2','V3','V4']):\n",
    "    plt.subplot(2,2,i+1)\n",
    "    sns.kdeplot(data=df, x=col, hue='Class', fill=True, common_norm=False, alpha=0.5, palette=['skyblue', 'purple']) \n",
    "    plt.title(f\"Distribution of {col} by Class\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b87960be-7396-4cae-8105-ab379beff522",
   "metadata": {},
   "source": [
    "***<h2>Phase 4: Preprocessing***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d830342d-634f-4c52-b13f-ef99838641ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardizes the \"Amount\" feature to improve model training efficiency and stability.  \n",
    "# Drops \"Time\" and raw \"Amount\" since they do not contribute significantly after scaling.  \n",
    "\n",
    "scaler = StandardScaler()\n",
    "df['Amount_scaled'] = scaler.fit_transform(df[['Amount']])\n",
    "df = df.drop(['Time','Amount'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "691b5866-759b-4113-bc7d-a9c9ca86ecf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separates dataset into features (X) for model input and target (y) representing fraud vs non-fraud.  \n",
    "# This prepares the data for training and evaluation of the classification model.  \n",
    "\n",
    "X = df.drop('Class', axis=1)\n",
    "y = df['Class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1789d57-51df-4323-af62-c49df2e68df1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splits the dataset into training (80%) and testing (20%) sets while preserving the fraud-to-legit ratio using stratify.  \n",
    "# Ensures the model sees balanced class proportions during training and evaluation, preventing bias.  \n",
    "# Important: Stratification maintains the rare fraud cases in both train and test sets for reliable performance assessment.  \n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "print(\"Training Data:\", X_train.shape, \"Testing Data:\", X_test.shape,\"\\n\")\n",
    "print(\"Fraud % in train:\", y_train.mean()*100, \"\\nFraud % in test:\", y_test.mean()*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba23e2b8-2930-482e-ab6c-d8a209c46225",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert y_train to a 1D NumPy array(if required) to avoid shape issues during class weight computation.  \n",
    "y_train = np.array(y_train).ravel()\n",
    "\n",
    "# Computes class weights automatically to handle severe imbalance between fraud (1) and non-fraud (0).  \n",
    "class_weights = class_weight.compute_class_weight(\n",
    "    class_weight = 'balanced',\n",
    "    classes = np.unique(y_train),                # Must match the unique labels in y_train\n",
    "    y = y_train\n",
    ")\n",
    "\n",
    "# Generates a dictionary mapping each class (0 and 1) to its weight, giving more importance to rare fraud cases.\n",
    "class_weights_dict = dict(zip(np.unique(y_train), class_weights))\n",
    "print(\"Class Weights:\\n\", class_weights_dict)\n",
    "\n",
    "# Important: These weights are later passed to model training so that the ANN does not ignore minority fraud samples."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df9e2321-42b1-4d1b-a599-c89c8728ceb6",
   "metadata": {},
   "source": [
    "***<h2>Phase 5: ANN model training***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cefe44d-c820-4f73-aaf4-cc252a0bac76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Builds the Artificial Neural Network (ANN) using a Sequential model with stacked layers.  \n",
    "# Input Layer: Takes features from training data, first Dense layer has 32 neurons with ReLU activation.  \n",
    "# Dropout Layers: Randomly drop 30% of neurons during training to reduce overfitting.  \n",
    "# Hidden Layer: Second Dense layer with 16 neurons and ReLU activation for deeper feature learning.  \n",
    "# Output Layer: Single neuron with sigmoid activation outputs probability of fraud (1) or non-fraud (0).  \n",
    "\n",
    "model = Sequential([\n",
    "    Dense(32, activation='relu', input_shape=(X_train.shape[1],)),\n",
    "    Dropout(0.3),\n",
    "    Dense(16, activation='relu'),\n",
    "    Dropout(0.3),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Compiles the model using Adam optimizer, binary crossentropy loss (for classification), and accuracy as the metric.  \n",
    "model.compile(optimizer=Adam(learning_rate=0.001), loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e1a6a38-78ca-48f8-a345-8584e6a82291",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03dc143d-901a-4860-ad25-ac80a4c8e178",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trains the ANN model on training data with class weights applied to handle imbalance (fraud vs non-fraud).  \n",
    "# Uses 20 epochs and large batch size (2048) for efficient training on big data.  \n",
    "# validation_split = 0.2 reserves 20% of training data for validation to monitor overfitting during training.  \n",
    "# class_weight ensures the rare fraud cases are given higher importance so the model learns to detect them.  \n",
    "# verbose = 1 shows detailed progress of training (loss and accuracy per epoch).  \n",
    "\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=20,\n",
    "    batch_size=2048,\n",
    "    validation_split=0.2,\n",
    "    class_weight=class_weights_dict,\n",
    "    verbose=1 )\n",
    "\n",
    "# Saves the trained model as 'fraud_model.keras' so it can be reused later without retraining.  \n",
    "model.save(\"fraud_model.keras\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "165d7770-c317-4c09-8a20-39b8eb471eb3",
   "metadata": {},
   "source": [
    "***<h2>Phase 6: Model Evaluation***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ffb5bd7-8fa4-4532-9990-36829eed1b47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions on test data (probabilities between 0 and 1)\n",
    "y_pred_prob = model.predict(X_test)\n",
    "\n",
    "# Convert predicted probabilities into binary classes (0 or 1) using threshold = 0.5\n",
    "y_pred = (y_pred_prob > 0.5).astype(int)\n",
    "\n",
    "# Classification Report - Print Precision, Recall, F1-score for both classes\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n",
    "\n",
    "#Calculate ROC-AUC score (area under ROC curve)\n",
    "roc_auc = roc_auc_score(y_test, y_pred_prob)\n",
    "print(\"ROC-AUC Score:\", roc_auc)\n",
    "\n",
    "# Important: threshold can be tuned (e.g., 0.3 or 0.7) to improve fraud detection\n",
    "#    -Recall for fraud class (1) is most critical since we must catch maximum fraud cases\n",
    "#    -AUC is a strong metric for imbalanced data, measures fraud vs non-fraud separability- \n",
    "#    -Display the ROC-AUC score (closer to 1 = better classification performance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6691cb19-ace6-4673-805e-6c134d9975ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute and visualise confusion matrix (rows = actual labels, columns = predicted labels)\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "plt.figure(figsize=(5,4))\n",
    "\n",
    "custom_cmap = sns.blend_palette(['skyblue', 'purple'], as_cmap=True) \n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap=custom_cmap, edgecolor= 'black')\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Actual\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcad8739-d863-48b1-832d-fb83e45464c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute and visualise PR AUC, precision and recall values at different probability thresholds\n",
    "\n",
    "prec, recall, _ = precision_recall_curve(y_test, y_pred_prob)\n",
    "pr_auc = auc(recall, prec)\n",
    "plt.plot(recall, prec, label=f'PR curve (AUC = {pr_auc:.4f})', color='purple')\n",
    "plt.xlabel(\"Recall\")\n",
    "plt.ylabel(\"Precision\")\n",
    "plt.title(\"Precision-Recall Curve\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "230544e7-a7af-444d-9e83-1d1bf16bedfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulated test labels and predicted probabilities\n",
    "\n",
    "np.random.seed(42)\n",
    "y_test_roc = np.random.randint(0, 2, 200)         \n",
    "y_pred_prob_roc = np.random.rand(200)            \n",
    "\n",
    "# Compute ROC curve and AUC\n",
    "fpr, tpr, thresholds = roc_curve(y_test_roc, y_pred_prob_roc)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "# Plot ROC Curve\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.plot(fpr, tpr, color='purple', lw=2, label=f'ROC curve (AUC = {roc_auc:.4f})') \n",
    "plt.plot([0,1], [0,1], color='skyblue', lw=2, linestyle='--')                          \n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbce41eb-52cf-4a5d-980a-92c3ba982572",
   "metadata": {},
   "source": [
    "<i>ROC Curve evaluates model performance by comparing True Positive Rate (TPR) vs False Positive Rate (FPR).  \n",
    "Important: TPR = ability to correctly identify fraud cases, FPR = incorrectly flagging legit cases as fraud.  \n",
    "\n",
    "<i>AUC (Area Under Curve) shows overall separability: closer to 1 = excellent model, 0.5 = random guessing.  \n",
    "Important: In imbalanced datasets (like fraud detection), ROC-AUC can sometimes look good even if fraud detection is poor.  \n",
    "\n",
    "<i>The baseline diagonal line represents a random classifier (no predictive power).  \n",
    "Important: A good model’s ROC curve should lie far above this diagonal.  \n",
    "\n",
    "<i>Practical note: For fraud detection, Precision-Recall Curve is often more informative than ROC, since it focuses on how well the model captures rare fraud cases without too many false alarms. ROC Curve evaluates model performance by comparing True Positive Rate (TPR) vs False Positive Rate (FPR).</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52451e6b-4d97-4553-a117-138c97feb67b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Loss & Accuracy Curves\n",
    "\n",
    "plt.figure(figsize=(12,5))\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(history.history['loss'], label='Train Loss', color='purple') \n",
    "plt.plot(history.history['val_loss'], label='Val Loss', color='skyblue') \n",
    "plt.legend(); plt.title(\"Loss Over Epochs\")\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(history.history['accuracy'], label='Train Acc', color='purple') \n",
    "plt.plot(history.history['val_accuracy'], label='Val Acc', color='skyblue') \n",
    "plt.legend(); plt.title(\"Accuracy Over Epochs\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a18deb8-0766-43cf-af38-80cd7b7b0518",
   "metadata": {},
   "source": [
    "<i>Plots training vs validation loss across epochs to check how well the model is learning.\n",
    "Important: If validation loss increases while training loss decreases → model is overfitting.\n",
    "\n",
    "<i>Plots training vs validation accuracy across epochs to evaluate performance.\n",
    "Important: Both curves should ideally converge; large gaps indicate overfitting or underfitting.\n",
    "\n",
    "<i>Loss curve is more reliable than accuracy for imbalanced data (fraud detection),\n",
    "since accuracy can be misleading if the model predicts mostly non-fraud.\n",
    "\n",
    "<i>These plots help in tuning epochs, batch size, and dropout rate to improve generalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73f30eb3-a825-4fb7-b458-f59a44a0d936",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to test model with new unseen transaction\n",
    "# input_data: list of 30 features (V1-V28, Amount_scaled, etc.) Example: a single row transaction\n",
    "\n",
    "def predict_transaction(model, input_data):\n",
    "\n",
    "    # Converts the list into a NumPy array and reshapes it to match the model’s input format.\n",
    "    input_array = np.array(input_data).reshape(1,-1)    \n",
    "    \n",
    "    prediction_prob = model.predict(input_array)[0][0]\n",
    "    prediction = int(prediction_prob > 0.5)\n",
    "    print(\"Prediction Probability:\", prediction_prob)\n",
    "    print(\"Transaction Classified as:\", \"FRAUD\" if prediction==1 else \"LEGITIMATE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cd892c5-95b8-4b57-a4b2-c9f2e3083a9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example Usage:\n",
    "# Taking first test transaction\n",
    "sample = X_test.iloc[0].values\n",
    "predict_transaction(model, sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcf36636-f1d8-4d5a-b318-bf6583b26568",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample 1: Legitimate Transaction (random values from test set)\n",
    "\n",
    "sample_legit = X_test[y_test==0].iloc[0].values\n",
    "print(\"Testing with a LEGITIMATE transaction\\n\\n\")\n",
    "predict_transaction(model, sample_legit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6620678-9d44-454a-af4a-a3fa2ed8b333",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample 2: Fraudulent Transaction (random values from test set)\n",
    "\n",
    "sample_fraud = X_test[y_test==1].iloc[0].values\n",
    "print(\"\\nTesting with a FRAUD transaction\\n\\n\")\n",
    "predict_transaction(model, sample_fraud)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c308339-155e-4d13-b13d-6b657735c32d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10 (tfenv)",
   "language": "python",
   "name": "tfenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
